{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization Assignment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting Set Up\n",
    "You should have cloned the GitHub Classroom link to receive the deliverables\n",
    "for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and do not edit this magic cell. \n",
    "# It helps getting things to work throughout the Jupyter notebook - in particular importing changes in functions made in files other than this workspace.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Particle Filter\n",
    "First, you will complete a series of quick exercises which will guide you through implementing a simplified particle filter. You will be given two files:\n",
    "\n",
    "    student_particle_filter.py\n",
    "    animate_particle_filter.py\n",
    "\n",
    "In `student_particle_filter` you will implement a particle filter which causes a set of randomly generated points on a 2d plane to converge on a specific point. `student_particle_filter` will write the particles' poses to a text file, which will be read and used to generate an animation.\n",
    "\n",
    "Detailed instructions for each step are in the comments of `student_particle_filter`.\n",
    "\n",
    "### 1. Setup\n",
    "\n",
    "Define a `Particle` class to represent the particles in the filter. Each particle should store its position ($x$,$y$) and its weight.\n",
    "\n",
    "Define a `ParticleSet` class to store the set of particles, the desired pose, and the methods which operate on the particle set.  Create an `__init__` method which takes the number of particles as input and creates a set of particles at random positions.\n",
    "\n",
    "### 2. Motion\n",
    "\n",
    "Implement a method for the `ParticleSet` class which adds some random Gaussian noise to the $x$ and $y$ positions of each particle in the filter. Be sure that the noise is different for each particle.\n",
    "\n",
    "*Hint:* try `numpy.random.normal`.\n",
    "\n",
    "### 3. Measurement Update\n",
    "\n",
    "Implement a method for the `ParticleSet` class which sets the weight of each particle inversely proportional to the particle's distance from the desired pose.\n",
    "\n",
    "### Testing your code\n",
    "\n",
    "Try running your code! If it works properly, the particle poses should be written to a file\n",
    "called `particle_filter_data.txt`. You can then run the cells below to view\n",
    "an animation of your particle filter converging on the desired pose which you set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify <yourGitHubName> to make this the path to your exercise folder\n",
    "EXERCISE_DIRECTORY = \"/code/dd21-localization-lx/packages/project-localization-slam-2019-<yourGitHubName>/\"\n",
    "import sys\n",
    "\n",
    "sys.path.append(EXERCISE_DIRECTORY)\n",
    "\n",
    "POSES_FILE_NAME = 'particle_filter_data.txt'\n",
    "POSES_FILE_PATH = EXERCISE_DIRECTORY + POSES_FILE_NAME\n",
    "\n",
    "NUM_PARTICLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from student_particle_filter import ParticleSet\n",
    "\n",
    "particles = ParticleSet(NUM_PARTICLES)\n",
    "\n",
    "try:\n",
    "    pp = open(POSES_FILE_PATH, 'w')\n",
    "    for _ in range(800):\n",
    "        particles.particle_motion()\n",
    "        particles.weight_particles()\n",
    "        particles.resample_particles()\n",
    "\n",
    "        for i in range(particles.p_list_weight.size):\n",
    "            pp.write(str(particles.p_list_x[i]) +\n",
    "                     '\\n' + str(particles.p_list_y[i]) + '\\n')\n",
    "finally:\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from functools import partial\n",
    "# from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from particle_filter.animate_particle_filter import read_pose_file\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "poses = read_pose_file(POSES_FILE_PATH,NUM_PARTICLES)\n",
    "\n",
    "line, = ax.plot([], [], 'bo',)\n",
    "\n",
    "ax.set_xlim(-10,10)\n",
    "ax.set_ylim(-10, 10)\n",
    "\n",
    "def animate(i,particles_trajectories,line):\n",
    "    particles_poses = particles_trajectories[i]\n",
    "    x_list = [pose[0] for pose in particles_poses]\n",
    "    y_list = [pose[1] for pose in particles_poses]\n",
    "\n",
    "    line = line.set_data(x_list,y_list)\n",
    "    return (line,)\n",
    "\n",
    "ani = FuncAnimation(fig,partial(animate,particles_trajectories=poses,line=line),frames=100,)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Optimizing the code\n",
    "\n",
    "Now that your filter is running, let's consider how we can optimize this process so that the localization particle filter will run quickly in real time on your drones.\n",
    "Python data structures and their operations are relatively slow compared to their Numpy counterparts because Numpy is written in C. You will use Numpy arrays to avoid storing the set of particle poses and their weights as lists of Python objects. You may comment out the `Particle` class entirely and replace the list of particle objects with two Numpy arrays for poses and weights stored in the `ParticleSet` class. Adjust the rest of the code accordingly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## OpenCV\n",
    "\n",
    "Now we that know the basics of how a particle filter uses weights and resampling to converge on a target, we need to address how to use OpenCV to estimate the motion and global position of the flying drone. To do this, you will complete the cells in this notebook using OpenCV functions to compute the translation in the plane between two drone poses, represented by two overlapping images taken on a real drone. You will be provided with the following files:\n",
    "\n",
    "    image_A.jpg\n",
    "    image_B.jpg\n",
    "    student_compute_displacement.py\n",
    "\n",
    "Keep in mind that the real-world dimensions of a pixel in the image will vary based on the height of the drone. Why is this?\n",
    "\n",
    "Your job is to write code in that will extract features from both images and compute a transformation between them. Use this transformation to compute the $x$,$y$, and yaw displacement *in meters* between the two images. This is exactly how you will implement the motion model for localization: we consider the meter displacement between two drone images to be the motion of the drone between the poses at which the images were taken.\n",
    "\n",
    "In this task, you will use OpenCV to implement the motion\n",
    "model for localization and SLAM on the drone. This involves computing the\n",
    "distance between overlapping camera frames taken by the drone as it flies.\n",
    "\n",
    "In this task, you will be expected to read OpenCV documentation in order to\n",
    "use the various functions properly. Please visit the [documentation](https://docs.opencv.org/3.4/index.html) and use the search bar to locate\n",
    "the indicated functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "image_A = cv2.imread('/code/dd21-localization-lx/assets/opencv/img_A.jpg')\n",
    "image_B = cv2.imread('/code/dd21-localization-lx/assets/opencv/img_B.jpg')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Extract Features\n",
    "\n",
    "In this step, we extract the coordinates of salient features from each image,\n",
    "called keypoints, as well as descriptors of those features. Look at the `detectAndCompute` function in OpenCV's documentation [here](https://docs.opencv.org/4.7.0/d0/d13/classcv_1_1Feature2D.html#a8be0d1c20b08eb867184b8d74c15a677).\n",
    "\n",
    "*Hint*: kp are keypoints, des are descriptors. Remember that each feature is\n",
    "described by a kp and a des.\n",
    "\n",
    "*Hint 2*: you may use `None` as the `mask` argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# TODO: use detector.detectAndCompute() (called ORB::Operator() in the documentation),\n",
    "# to extract keypoints and descriptors from both images\n",
    "plt.clf()\n",
    "\n",
    "NUM_FEATURES = 100\n",
    "detector = cv2.ORB.create(nfeatures=NUM_FEATURES, scoreType=cv2.ORB_FAST_SCORE)\n",
    "\n",
    "aKeyPoints, aDescriptors = detector.detectAndCompute(image_A, None)\n",
    "bKeyPoints, bDescriptors = detector.detectAndCompute(image_B, None)\n",
    "\n",
    "# Show the keypoints\n",
    "image_A_annotated = cv2.drawKeypoints(image_A, aKeyPoints, None, color=(0,255,0),flags= cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "image_B_annotated = cv2.drawKeypoints(image_B, bKeyPoints, None, color=(0,255,0),flags= cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "fig,(ax1, ax2), = plt.subplots(1,2,figsize=(16, 12))\n",
    "ax1.imshow(image_A_annotated)\n",
    "ax1.set_title(\"Features in Image A\")\n",
    "ax2.imshow(image_B_annotated)\n",
    "ax2.set_title(\"Features in Image B\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Match Features and Descriptors\n",
    "\n",
    "In this step, we compare the descriptors of features in each image to\n",
    "to find the features in each image which describe the same point.\n",
    "\n",
    "Search for `Descriptor Matcher` in the OpenCV documentation.\n",
    "\n",
    "Some questions to think about (your knowledge of these may be checked during a project checkoff):\n",
    "\n",
    "- How is the list of matches formatted?\n",
    "- What kind of objects are in the list?\n",
    "- What fields do they have?\n",
    "- Decide on a value for the parameter `k`. Why did you pick that value? (It may be helpful to do the rest of the comments in this step before answering this question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "index_params = dict(algorithm=6, table_number=6,\n",
    "                    key_size=12, multi_probe_level=1)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# TODO: use knnMatch to obtain the list of matches between the image_A\n",
    "# descriptors and the image_B descriptors\n",
    "matchesList = matcher.knnMatch(aDescriptors, bDescriptors, k=2)\n",
    "\n",
    "fig1,ax, = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Show the best 10 matched keypoints\n",
    "image_matches = cv2.drawMatchesKnn(image_A, aKeyPoints, image_B, bKeyPoints, matchesList, None, matchColor=(255,0,0),flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "ax.imshow(image_matches)\n",
    "ax.set_title('Matched Keypoints')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we will filter out the poor matches from the matches list\n",
    "\n",
    "*Hint*: a large distance between descriptors means a poor match, and a small\n",
    "distance indicates a strong match\n",
    "\n",
    "*Hint 2*: `knnMatch` finds the k best matches in the second input list for each\n",
    "descriptor in the first input list. Compare the best matches for each descriptor\n",
    "to judge the quality of a match. What does it mean when the best match is\n",
    "similar in quality to the second best match? What does it mean when the best\n",
    "match is much better than the second best match?\n",
    "\n",
    "Apply the ratio test, as per Lowe's paper:\n",
    "[Source 1](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf#page=20)\n",
    "[Source 2](https://docs.opencv.org/3.3.0/dc/dc3/tutorial_py_matcher.html)\n",
    "\n",
    "\n",
    "Then, we will recover the coordinates in each image of the\n",
    "features we have just matched. Each match corresponds to two features: one in\n",
    "`image_A`, and one in `image_B`, which both describe the same point.\n",
    "\n",
    "Some questions to think about (your knowledge of these may be checked during a project checkoff):\n",
    "\n",
    "- Search `Keypoint` in the documentation to find details on the keypoint object.\n",
    "- What field does the keypoint object have to hold feature coordinates?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: filter the matches based on their quality.\n",
    "\n",
    "good = []\n",
    "for m in matchesList:\n",
    "    # a few places in the list where I would expect a pair of matches there isn't a pair and that breaks things\n",
    "    if len(m) == 2:\n",
    "        if m[0].distance < 0.75*m[1].distance:\n",
    "            good.append(m[0])\n",
    "\n",
    "# TODO: Obtain lists of the keypoint coordinates from both image A and image B\n",
    "# for each of the filtered matches.\n",
    "\n",
    "image_A_match_good_keypoints = np.array([])\n",
    "image_B_match_good_keypoints = np.array([])\n",
    "\n",
    "for g in good:\n",
    "    image_A_match_good_keypoints = np.append(\n",
    "        image_A_match_good_keypoints, aKeyPoints[g.queryIdx])\n",
    "    image_B_match_good_keypoints = np.append(\n",
    "        image_B_match_good_keypoints, bKeyPoints[g.trainIdx])\n",
    "\n",
    "# print(image_A_match_good_keypoints)\n",
    "# print(image_B_match_good_keypoints)\n",
    "\n",
    "\n",
    "image_A_match_keypoints = cv2.KeyPoint_convert(image_A_match_good_keypoints)\n",
    "image_B_match_keypoints = cv2.KeyPoint_convert(image_B_match_good_keypoints)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Estimate the Transform\n",
    "In this step, we will use the lists of coordinates of the matching features\n",
    "from each image to compute a transformation between the two pictures.\n",
    "\n",
    "\n",
    "Use `estimateRigidTransform` or  `estimateAffinePartial2D` to find the transformation between `imageA_points` and `imageB_points`.\n",
    "\n",
    "*Tip*: Search `estimate transform` in the documentation.\n",
    "\n",
    "*Hint*: the 3rd argument to `estimateRigidTransform` should be `False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imageA_points = np.float32(image_A_match_keypoints).reshape(-1, 1, 2)\n",
    "imageB_points = np.float32(image_B_match_keypoints).reshape(-1, 1, 2)\n",
    "\n",
    "transform = cv2.estimateAffinePartial2D(imageA_points, imageB_points)[0]\n",
    "transform1 = cv2.estimateRigidTransform(imageA_points, imageB_points, False)\n",
    "\n",
    "print(transform)\n",
    "print(transform1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Convert to Meters\n",
    "In this step we will use the height of the drone to convert the pixel displacement\n",
    "from the transformation to meters.\n",
    "\n",
    "To do this, we multiply the height (meters) by a specific camera scale to obtain\n",
    "the meter-per-pixel value of the camera. This number varies with the height\n",
    "of the drone. Then, we can multiply this ratio by the pixel displacement (what you\n",
    "just computed) to find the meter displacement.\n",
    "\n",
    "Some questions to think about (your knowledge of these may be checked during a project checkoff):\n",
    "\n",
    "- What unit must the camera scale have?\n",
    "- How might you experimentally determine the camera scale for the drone's camera?\n",
    "- Bonus question: how might you mathematically determine the scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = <transformation computed with estimateRigidTransform >\n",
    "x_displacement = -transform[0, 2]\n",
    "y_displacement = transform[1, 2]\n",
    "yaw_displacement = -np.arctan2(transform[1, 0], transform[0, 0])\n",
    "\n",
    "# Compute the meter distance between the two image frames\n",
    "\n",
    "scale = 0.23/290\n",
    "print(x_displacement*scale)\n",
    "print(y_displacement*scale)\n",
    "print(yaw_displacement)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implement Localization on the Duckiedrone\n",
    "\n",
    "[TODO: TEST] \n",
    "\n",
    "We are now ready to implement localization on the drone.\n",
    "\n",
    "You will be given two files:\n",
    "\n",
    "    student_run_localization.py\n",
    "    student_localization_helper.py\n",
    "\n",
    "`student_run_localization` runs localization on the drone and is complete, you will not need to implement any code in that file. However, you may adjust the `NUM_PARTICLE` and `NUM_FEATURE` values to experiment with the speed/accuracy tradeoff concerning the number of particles in the filter and the number of features extracted by OpenCV. You may also edit this file if you need to change the map over which you want to localize.\n",
    "\n",
    "`student_localization_helper.py` contains the particle filter class and its methods. Many of the methods are not implemented. The docstrings describe the intended functionality of each function, and the `TODOs` indicate tasks to be completed. Your assignment is to follow the `TODOs` and implement the missing functionality of the particle filter. Much of the code you just wrote can be used here!\n",
    "\n",
    "__Tip__: we recommend that you read through the parts of the code which we are not asking you to implement,\n",
    "as this will help you to understand what is going on with the code and will likely save you\n",
    "debugging time. For example, we are not asking you to implement the methods `resample_particles` or `initialize_particles`\n",
    "for localization, but it might help you to understand how they work! The same goes for the SLAM project.\n",
    "\n",
    "Note that for both this part of the assignment and for SLAM, there is not any \"correct\" universal\n",
    "implementation of the code as long as your solutions work.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Set Up\n",
    "\n",
    "To run the porject on the drone, clone your repository (`project-localization-slam-2019-<yourGitHubUsername>`) in the `/catkin_ws/src` folder\n",
    "on your drone. \n",
    "\n",
    "You should find the `package.xml` and `CMakeLists.txt` files which you will need to modify\n",
    "to build the package:\n",
    "\n",
    "- On line `3` of `package.xml` you should replace\n",
    "`yourGithubName` with your GitHub name so it matches the name of your directory. \n",
    "- Do the same on line `2` of `CMakeLists.txt` \n",
    "\n",
    "Finally, you should navigate to the `/catkin_ws` folder and run:\n",
    "\n",
    "    catkin_make --pkg project-localization-slam-2019-yourGithubName\n",
    "\n",
    "to build your package so it is ros-runnable by `pidrone_pkg`. You should only need to\n",
    "do this step one time.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Testing\n",
    "To test the functionality of your localization code, you may fly the drone while running\n",
    "\n",
    "    rosrun project_localization_slam_2019_<yourGithubName> student_run_localization.py\n",
    "\n",
    "in the vision screen. \n",
    "\n",
    "It is easiest to take photos of the new map with a cell phone or other camera. Take them at a height of 25cm and use an image stitching software to generate the map. We recommend auto-stitch. Replace `map.jpg` with your new map and change the following four parameters in `student_run_localization.py`, `student_localization_helper.py`: `MAP_PIXEL_WIDTH`, `MAP_PIXEL_HEIGHT`, `MAP_REAL_WIDTH`, `MAP_REAL_HEIGHT`. You may need to resize the image to be smaller if it is too large.\n",
    "\n",
    "You should see poses printed out which correspond to the drone's position over the map.\n",
    "\n",
    "You may also use the visualization functions in [the previous sections](#particle-filter) to visualize the animation of your particle filter. Print the ($x$,$y$) pose of each particle on separate lines in a text file to point at using the variable `POSES_FILE_NAME`. Make sure you adjust `NUM_PARTICLES` to reflect the number of particles you are using! (using the visualizer is optional)\n",
    "\n",
    "### Checkoff\n",
    "We will verify that your code has the following functionality:\n",
    "\n",
    " 1. You can run `student_run_localization.py` and take off with your drone.\n",
    " 2. While flying, you can hit 'r' and the poses will begin printing to the terminal. You can hit 'r' again and localization will restart.\n",
    " 3. While flying, you can hit 'p' to toggle position hold on and off.\n",
    " 4. Run `student_run_localization.py` while holding the drone over a mapped area. Do not arm the drone. As you move the drone around, verify that the poses reflect the movement. Verify visually that the poses are close to the actual position of the drone in the map. For example, if you are holding the drone above the bottom left corner of the mapped area, the pose should be close to (0,0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
